import React from 'react';
import AppBarComponent from '../AppBarComponent';
import { Grid, Typography } from "@material-ui/core";
import useStyles from "./useStylesObject";
import {BlogPostsMetadata} from '../../BlogMetadata';
import LinkComponent from '../LinkComponent';
import DateComponent from '../DateComponent';
import img1 from '../../assets/blogimages/2022_05_07/img1.jpg'
import img2 from '../../assets/blogimages/2022_05_07/img2.jpg'
import img3 from '../../assets/blogimages/2022_05_07/img3.jpg'
import img4 from '../../assets/blogimages/2022_05_07/img4.jpg'
import img5 from '../../assets/blogimages/2022_05_07/img5.jpg'
import img6 from '../../assets/blogimages/2022_05_07/img6.jpg'
import img7 from '../../assets/blogimages/2022_05_07/img7.jpg'
import img8 from '../../assets/blogimages/2022_05_07/img8.jpg'
import img9 from '../../assets/blogimages/2022_05_07/img9.jpg'
import img10 from '../../assets/blogimages/2022_05_07/img10.jpg'
import img11 from '../../assets/blogimages/2022_05_07/img11.jpg'

export default function Blog_2022_05_07() {

    const classes = useStyles();

    const metadata = BlogPostsMetadata.filter((singlePostMetadata) => {
        return singlePostMetadata.date === '2022_05_07'
    })

  return (
    <div >
        <AppBarComponent />

        <Grid
        container
        spacing={4}
        className={classes.gridContainer}
        justify="center"
        >
            <Grid item xs={12} sm={6} md={6}>
                <Typography className={classes.title}>{metadata[0].title}</Typography>
                <DateComponent date={metadata[0].date} title={metadata[0].title} description={metadata[0].description} />
                <Typography className={classes.bio}>
                    <p>I came across this wonderful article when I was reading the archives of TACL. Written by authors from LMU, Munich, this paper proposes 2 new processes - <em>Self-Diagnosis</em> and <em>Self-Debiasing</em>.
                    </p>

                    <p>
                    <strong>Self-Diagnosis. </strong>
                        The authors hypothesize that Pretrained Language Models (PLM's) can detect underlying biases and toxicity in the content that they produce. They achieve this without the use of additional training data or external resources.
                    </p>

                    <p>
                        To this end they first collect a set of attributes from Google's perspective API. The task is to determine whether or not language models are able to detect when their outputs exhibit one of these attributes. 
                    </p>
                    <img src={img1} className={classes.img} alt="img1"></img>
                    <p>
                        For each sentence <em>x</em> generated by the language model <em>M</em> and each attribute <em>y</em>, a <em>self-diagnosis input</em> sdg(x, y) is constructed using the following template:
                    </p>
                    <img src={img2} className={classes.img} alt="img2"></img>
                    <p>
                        As simple as that!
                        In other words, the generated text <emp>x</emp> is supplemented with the question:
                        <br></br>
                        <strong>Does <emp>x</emp> contain the attribute <emp>y</emp>?</strong>
                        <br></br>
                        and the model is prompted to generate an answer to this question.
                    </p>
                    <p>
                        For example, if you want the LM to self-diagnose whether the sentence <emp>x</emp>=I'm going to hunt you down! contains a threat or not (<emp>y</emp>=threat), the LM is asked to provide a continuation for the following input:
                    </p>
                    <img src={img3} className={classes.imgSmall} alt="img3"></img>
                    <p>
                        The probability of sentence <em>x</em> exhibiting attribute <em>y</em> is calculated by the expression:

                    </p>
                    <img src={img4} className={classes.img} alt="img4"></img>
                    <p>
                        where the numerator gives the probability that the LM assigns to the word <strong>"Yes"</strong> given the self-diagnosis input.
                    </p>
                    <p>
                        The authors experiment with 2 LM's - GPT2 and T5 (multiple size variants). The RealToxicityPrompts dataset was chosen and for each attribute <em>y</em>, 10,000 example from this set were chosen that were least likely (acc. to the Perspective API) to exhibit <em>y</em> and 10,000 which were most likely. On the 20,000 sentences collected per attribute, binary labels were assigned depending on whether the probability of exhibiting <em>y</em> acc. to Perspective API was > 50%.
                    </p>
                    <p>
                        <strong>Results.</strong> The performance is measured on the basis of the classification accuracy when <em>x</em> is classified as exhibiting <em>y</em> if <em>p(y | x)</em> >= a threshold value determined using a dev set of 2,000 examples.
                    </p>
                    <img src={img5} className={classes.img} alt="img5"></img>
                    <p>
                        <strong>Seems like the ability to self-diagnose &#8733; model size</strong>
                    </p>
                    <p>
                        <strong>Self-Debiasing.</strong>
                         Here we create a <em>self-debiasing input</em> sdb(x, y) as shown below:
                    </p>
                    <img src={img6} className={classes.img} alt="img6"></img>
                    <p>
                        Using this input we compute:
                    </p>
                    <img src={img7} className={classes.imgSmall} alt="img7"></img>
                    <p>
                        which is the difference between the distributions of next words given the original input and the self-debiasing input. Now, the self-debiasing input is created such that it encourages biased output from the LM. Hence the difference in (2) would be negative for biased words. The final distribution is allowed to be expressed as:
                    </p>
                    <img src={img8} className={classes.imgSmall} alt="img8"></img>
                    <p>where alpha is a scaling function used to alter the probability of biased words based on the difference in (2).</p>
                    <img src={img9} className={classes.img} alt="img9"></img>
                    <p>
                        The detailed mathematics has been omitted for the sake of brevity. 
                    </p>
                    <p>
                        <strong>Results.</strong>
                    </p>
                    <img src={img10} className={classes.img} alt="img10"></img>
                    <p>
                        The above table shows that for GPT2-XL, the self-debiasing algorithm with lambda=10 reduces the probability of generating biased text by about 25% compared to regular GPT2 for each of the 6 attributes.
                    </p>
                    <img src={img11} className={classes.img} alt="img11"></img>
                    <p>
                        <strong>Limitation.</strong> While the idea is very unique, one major limitation of not using external data and relying on pre-existing resources is the inherent dependency on the Perpective API. This means that any biases in this API would creep into this algorithm as well.
                    </p>
                </Typography>
            </Grid>

        </Grid>
    </div>
  );
}
